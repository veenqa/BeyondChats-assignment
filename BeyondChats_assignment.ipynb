{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO/9Hp/9bQE6dVgzZPDQnIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veenqa/BeyondChats-assignment/blob/main/BeyondChats_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"REDDIT_CLIENT_ID\"] = \"cSpqJb9vbsmBZnnwjayzpA\"\n",
        "os.environ[\"REDDIT_CLIENT_SECRET\"] = \"ecpKWZB5xNzDC91D0gYyaOyzJ8mjYA\"\n",
        "os.environ[\"REDDIT_USER_AGENT\"] = \"PersonaGenerator2025/1.0 by Ok-Respect3852\"\n"
      ],
      "metadata": {
        "id": "2zHeoR4Angvm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import textwrap\n",
        "from datetime import datetime\n",
        "\n",
        "import praw\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "REDDIT_CLIENT_ID     = os.getenv(\"REDDIT_CLIENT_ID\")\n",
        "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
        "REDDIT_USER_AGENT    = os.getenv(\"REDDIT_USER_AGENT\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_username(url_or_user):\n",
        "    url_or_user = url_or_user.strip().rstrip(\"/\")\n",
        "    if url_or_user.startswith(\"u/\") or url_or_user.startswith(\"user/\"):\n",
        "        return url_or_user.split(\"/\", 1)[1]\n",
        "    m = re.search(r\"reddit\\.com\\/(?:u|user)\\/([^\\/]+)\", url_or_user, re.IGNORECASE)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    if re.fullmatch(r\"[A-Za-z0-9_-]+\", url_or_user):\n",
        "        return url_or_user\n",
        "    return None\n",
        "\n",
        "\n",
        "def initialize_reddit():\n",
        "    if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT]):\n",
        "        print(\"‚ùå Set Reddit API credentials as environment variables.\")\n",
        "        sys.exit(1)\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=REDDIT_CLIENT_ID,\n",
        "        client_secret=REDDIT_CLIENT_SECRET,\n",
        "        user_agent=REDDIT_USER_AGENT,\n",
        "        check_for_async=False\n",
        "    )\n",
        "    return reddit\n",
        "\n",
        "\n",
        "def scrape_user_data(reddit, username):\n",
        "    print(f\"üîç Scraping Reddit user: u/{username}\")\n",
        "    data = []\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        _ = user.id\n",
        "    except:\n",
        "        print(\"‚ùå Could not access profile.\")\n",
        "        return []\n",
        "\n",
        "    for c in user.comments.new(limit=100):\n",
        "        if c.body not in ['[deleted]', '[removed]']:\n",
        "            data.append(f\"Comment in r/{c.subreddit.display_name}: {c.body.strip()}\")\n",
        "    for s in user.submissions.new(limit=50):\n",
        "        text = s.title + \"\\n\\n\" + s.selftext if s.selftext else s.title\n",
        "        if text and text not in ['[deleted]', '[removed]']:\n",
        "            data.append(f\"Post in r/{s.subreddit.display_name}: {text.strip()}\")\n",
        "\n",
        "    print(f\"‚úÖ Scraped {len(data)} total items.\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_hf_model():\n",
        "    print(\"üß† Loading summarization model‚Ä¶\")\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "    return summarizer\n",
        "\n",
        "\n",
        "def generate_persona(summarizer, texts, username):\n",
        "    print(\"üß† Generating persona‚Ä¶\")\n",
        "    combined = \"\\n\\n\".join(texts[:30])\n",
        "    chunk = combined[:1024]  # Truncate if needed\n",
        "    summary = summarizer(chunk, max_length=250, min_length=80, do_sample=False)[0]['summary_text']\n",
        "\n",
        "    persona = f\"\"\"Reddit User Persona: u/{username}\n",
        "Generated using a Hugging Face summarization model.\n",
        "\n",
        "Summary of Behavior:\n",
        "{summary.strip()}\n",
        "\n",
        "Note: This persona was inferred from Reddit posts/comments.\n",
        "\"\"\"\n",
        "    return persona\n",
        "\n",
        "\n",
        "def save_persona(username, persona_text):\n",
        "    filename = f\"{username}_persona.txt\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(persona_text)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"üîµ Reddit Persona Generator (Hugging Face)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    profile = input(\"Enter Reddit profile URL or username: \").strip()\n",
        "    username = extract_username(profile)\n",
        "    if not username:\n",
        "        print(\"‚ùå Invalid Reddit username.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    reddit = initialize_reddit()\n",
        "    data = scrape_user_data(reddit, username)\n",
        "    if not data:\n",
        "        print(\"‚ùå No content found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    summarizer = load_hf_model()\n",
        "    persona = generate_persona(summarizer, data, username)\n",
        "    file = save_persona(username, persona)\n",
        "\n",
        "    print(f\"\\nüíæ Saved to: {file}\")\n",
        "    print(\"\\nüìù Preview:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(textwrap.indent(persona[:600], \"  \"))\n",
        "    print(\"‚úÖ Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204dbfbc-b75b-42c6-adea-ffac6b78adc3",
        "id": "HU-e4Pcxn8lX"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîµ Reddit Persona Generator (Hugging Face)\n",
            "============================================================\n",
            "Enter Reddit profile URL or username: https://www.reddit.com/user/Hungry-Move-6603/\n",
            "üîç Scraping Reddit user: u/Hungry-Move-6603\n",
            "‚úÖ Scraped 12 total items.\n",
            "üß† Loading summarization model‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Generating persona‚Ä¶\n",
            "\n",
            "üíæ Saved to: Hungry-Move-6603_persona.txt\n",
            "\n",
            "üìù Preview:\n",
            "------------------------------------------------------------\n",
            "  Reddit User Persona: u/Hungry-Move-6603\n",
            "  Generated using a Hugging Face summarization model.\n",
            "\n",
            "  Summary of Behavior:\n",
            "  Comment in r/nagpur: I was caught without helmet and license (close to my home). Cops outright wanted to fine me, but a 'common guy' came in and discussed bribe on my behalf with cops . I gave him 200rs (back in 2011) and Cops let me go . He was not a common man. He was their agent and shield. If I or anyone recorded myself handing out money, it would be to a common . man, not cops directly .\n",
            "\n",
            "  Note: This persona was inferred from Reddit posts/comments.\n",
            "\n",
            "‚úÖ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import textwrap\n",
        "from datetime import datetime\n",
        "\n",
        "import praw\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "REDDIT_CLIENT_ID     = os.getenv(\"REDDIT_CLIENT_ID\")\n",
        "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
        "REDDIT_USER_AGENT    = os.getenv(\"REDDIT_USER_AGENT\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_username(url_or_user):\n",
        "    url_or_user = url_or_user.strip().rstrip(\"/\")\n",
        "    if url_or_user.startswith(\"u/\") or url_or_user.startswith(\"user/\"):\n",
        "        return url_or_user.split(\"/\", 1)[1]\n",
        "    m = re.search(r\"reddit\\.com\\/(?:u|user)\\/([^\\/]+)\", url_or_user, re.IGNORECASE)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    if re.fullmatch(r\"[A-Za-z0-9_-]+\", url_or_user):\n",
        "        return url_or_user\n",
        "    return None\n",
        "\n",
        "\n",
        "def initialize_reddit():\n",
        "    if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT]):\n",
        "        print(\"‚ùå Set Reddit API credentials as environment variables.\")\n",
        "        sys.exit(1)\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=REDDIT_CLIENT_ID,\n",
        "        client_secret=REDDIT_CLIENT_SECRET,\n",
        "        user_agent=REDDIT_USER_AGENT,\n",
        "        check_for_async=False\n",
        "    )\n",
        "    return reddit\n",
        "\n",
        "\n",
        "def scrape_user_data(reddit, username):\n",
        "    print(f\"üîç Scraping Reddit user: u/{username}\")\n",
        "    data = []\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        _ = user.id\n",
        "    except:\n",
        "        print(\"‚ùå Could not access profile.\")\n",
        "        return []\n",
        "\n",
        "    for c in user.comments.new(limit=100):\n",
        "        if c.body not in ['[deleted]', '[removed]']:\n",
        "            data.append(f\"Comment in r/{c.subreddit.display_name}: {c.body.strip()}\")\n",
        "    for s in user.submissions.new(limit=50):\n",
        "        text = s.title + \"\\n\\n\" + s.selftext if s.selftext else s.title\n",
        "        if text and text not in ['[deleted]', '[removed]']:\n",
        "            data.append(f\"Post in r/{s.subreddit.display_name}: {text.strip()}\")\n",
        "\n",
        "    print(f\"‚úÖ Scraped {len(data)} total items.\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_hf_model():\n",
        "    print(\"üß† Loading summarization model‚Ä¶\")\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "    return summarizer\n",
        "\n",
        "\n",
        "def generate_persona(summarizer, texts, username):\n",
        "    print(\"üß† Generating persona‚Ä¶\")\n",
        "    combined = \"\\n\\n\".join(texts[:30])\n",
        "    chunk = combined[:1024]  # Truncate if needed\n",
        "    summary = summarizer(chunk, max_length=250, min_length=80, do_sample=False)[0]['summary_text']\n",
        "\n",
        "    persona = f\"\"\"Reddit User Persona: u/{username}\n",
        "Generated using a Hugging Face summarization model.\n",
        "\n",
        "Summary of Behavior:\n",
        "{summary.strip()}\n",
        "\n",
        "Note: This persona was inferred from Reddit posts/comments.\n",
        "\"\"\"\n",
        "    return persona\n",
        "\n",
        "\n",
        "def save_persona(username, persona_text):\n",
        "    filename = f\"{username}_persona.txt\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(persona_text)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"üîµ Reddit Persona Generator (Hugging Face)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    profile = input(\"Enter Reddit profile URL or username: \").strip()\n",
        "    username = extract_username(profile)\n",
        "    if not username:\n",
        "        print(\"‚ùå Invalid Reddit username.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    reddit = initialize_reddit()\n",
        "    data = scrape_user_data(reddit, username)\n",
        "    if not data:\n",
        "        print(\"‚ùå No content found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    summarizer = load_hf_model()\n",
        "    persona = generate_persona(summarizer, data, username)\n",
        "    file = save_persona(username, persona)\n",
        "\n",
        "    print(f\"\\nüíæ Saved to: {file}\")\n",
        "    print(\"\\nüìù Preview:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(textwrap.indent(persona[:600], \"  \"))\n",
        "    print(\"‚úÖ Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l_6FRuiopN-",
        "outputId": "f3e77418-8463-4b6d-ca85-c1b78e53262b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîµ Reddit Persona Generator (Hugging Face)\n",
            "============================================================\n",
            "Enter Reddit profile URL or username: https://www.reddit.com/user/gallowboob/\n",
            "üîç Scraping Reddit user: u/gallowboob\n",
            "‚úÖ Scraped 150 total items.\n",
            "üß† Loading summarization model‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Generating persona‚Ä¶\n",
            "\n",
            "üíæ Saved to: gallowboob_persona.txt\n",
            "\n",
            "üìù Preview:\n",
            "------------------------------------------------------------\n",
            "  Reddit User Persona: u/gallowboob\n",
            "  Generated using a Hugging Face summarization model.\n",
            "\n",
            "  Summary of Behavior:\n",
            "  It would be too noisy to go ham on this account. I could find a trusted surrogate user to feed content through and not bother making an alt (i'm jk... unless?) I didn‚Äôt think I could love anything this much. She‚Äôs unreal alright. I‚Äôm happy to see this comment! I've always been  a fan of reddit. It looks like SOMETHING is finally happening. Maybe I never left .\n",
            "\n",
            "  Note: This persona was inferred from Reddit posts/comments.\n",
            "\n",
            "‚úÖ Done.\n"
          ]
        }
      ]
    }
  ]
}